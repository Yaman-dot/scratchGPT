{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a39fd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Synchronous CUDA errors\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"   # Device-side assertions\n",
    "device = 'cuda:0'  # Explicitly use first GPU\n",
    "print(f\"Using {device} device\")\n",
    "# HYPER PARAMETERS\n",
    "block_size = 32\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 3e-4\n",
    "hidden_size = 128\n",
    "dropout = 0.2\n",
    "n_layer = 4\n",
    "n_head = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b44e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()  # Ensure all CUDA operations are complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156acbc",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6222d7c-e33f-4a2f-a494-9e287c810118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filePATH):\n",
    "    with open(filePATH, 'r', encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "def train_val_split(data, split):\n",
    "    n = int(split*len(data))\n",
    "    return data[:n], data[n:]\n",
    "\n",
    "def get_batch(split, train_data, val_data):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).long()\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).long()\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "#Debugging\n",
    "def print_progress(epoch, epochs, i, num_batches, loss):\n",
    "    progress = int((i + 1) / num_batches * 30)  # bar length = 30\n",
    "    bar = \"█\" * progress + \"-\" * (30 - progress)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | [{bar}] {i+1}/{num_batches} \"\n",
    "        f\"Loss: {loss:.4f}\",\n",
    "        end=\"\\r\",\n",
    "        flush=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a7fb5",
   "metadata": {},
   "source": [
    "Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb29e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "text = read_file(\"/kaggle/input/wiz-of-oz/wiz_of_oz.txt\")\n",
    "#print(f\"Length of dataset in characters: {len(text)}\")\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a3d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 51, 33, 65, 65, 74, 72, 73, 71, 54, 73, 62, 68, 67, 22,  1,\n",
      "        28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47, 33,\n",
      "        50, 25, 42, 28, 52,  0,  0,  1,  1, 51, 33, 65, 65, 74, 72, 73, 71, 54,\n",
      "        73, 62, 68, 67, 22,  1, 40, 33, 27, 35, 33, 38, 31,  1, 44, 32, 29,  1,\n",
      "        40, 42, 33, 38, 27, 29, 43, 43, 11, 52,  0,  0,  0,  0,  0,  1,  1, 28,\n",
      "        39, 42, 39, 44, 32, 49,  1, 25, 38, 28])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b9102d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_val_split(data, 0.8)\n",
    "x, y = get_batch(\"train\", train_data, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381197dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value in train_data: 80\n",
      "Vocab size: 81\n"
     ]
    }
   ],
   "source": [
    "print(\"Max value in train_data:\", train_data.max().item())\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "943057f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(hidden_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(hidden_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(hidden_size, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size, dtype=torch.bool)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,head_size)\n",
    "        q = self.query(x) #(B,T,head_size)\n",
    "        v = self.value(x) #(B,T,head_size)\n",
    "                                                                    #we square root this to prevent large dot product values\n",
    "        attn_weights = q @ k.transpose(-2, -1) * k.shape[-1]** -0.5 #(B,T,T)\n",
    "        attn_weights = attn_weights.masked_fill(~self.tril[:T, :T], float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1) #(B,T,T)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = attn_weights @ v #(B,T,head_size)\n",
    "        return attn_output\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head):\n",
    "        super().__init__()\n",
    "        assert hidden_size % n_head == 0\n",
    "        self.head_size = hidden_size // n_head\n",
    "        self.n_head = n_head\n",
    "        self.heads = nn.ModuleList([Head(hidden_size,self.head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # Process each head in parallel\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        attn_output = torch.cat(head_outputs, dim=-1)  # (B, T, hidden_size)\n",
    "        attn_output = self.proj(attn_output)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return attn_output\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head):\n",
    "        super().__init__()\n",
    "        head_size = hidden_size // n_head\n",
    "        self.attn = MultiHeadAttention(hidden_size, n_head)\n",
    "        self.ffwd = FeedForward(hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "    def forward(self, x):\n",
    "        x_ln1 = self.ln1(x)\n",
    "        attn_output = self.attn(x_ln1)\n",
    "        x = x + attn_output  # Residual connection\n",
    "        x_ln2 = self.ln2(x)\n",
    "        ffwd_output = self.ffwd(x_ln2)\n",
    "        x = x + ffwd_output  # Residual connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, hidden_size) #Token embeddings\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, hidden_size) #Positional embeddings\n",
    "        self.blocks = nn.Sequential(*[Block(hidden_size, n_head=n_head) for _ in range(n_layer)]) #Stack of transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(hidden_size) #Final layer norm\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size) #Language model head\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        index = torch.clamp(index, 0, vocab_size-1)\n",
    "\n",
    "        B, T = index.shape\n",
    "        tok_emb = self.token_embedding_table(index)  # B,T,C\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T, device=index.device))  # T,C\n",
    "        x = tok_emb + pos_emb  # B,T,C\n",
    "        x = self.blocks(x)  # B,T,C\n",
    "        x = self.ln_f(x)  # B,T,C\n",
    "        logits = self.lm_head(x) # B,T,vocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range (max_new_tokens):\n",
    "            index = index[:, -block_size:]\n",
    "            logits, loss = self.forward(index) #get predictions\n",
    "            logits = logits[:, -1, :] #Becomes B, C\n",
    "            probs = F.softmax(logits, dim=-1) #get probabilities\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "            index_next = torch.clamp(index_next, 0, self.token_embedding_table.num_embeddings - 1) # Clamp to valid range to prevent CUDA assert\n",
    "            index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5317995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 21 07:57:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P0             30W /  250W |     517MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645cee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size, hidden_size, dropout)\n",
    "\n",
    "# 2️⃣ Test CPU forward first\n",
    "dummy_index = torch.randint(0, vocab_size, (1, 10))\n",
    "logits, loss = model(dummy_index)\n",
    "\n",
    "# 3️⃣ Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# 4️⃣ Wrap in DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# 5️⃣ Prepare GPU input\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# 6️⃣ Generate text safely\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    generated_indices = model.module.generate(context, max_new_tokens=500)\n",
    "else:\n",
    "    generated_indices = model.generate(context, max_new_tokens=500)\n",
    "generatedChars = decode(model.generate(context, max_new_tokens=500)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce4b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5'UCYuBJ9'vQwO﻿Ez-TIN:*&!qNs;x3 H\n"
     ]
    }
   ],
   "source": [
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f347b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size, hidden_size, dropout).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de8e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_data, model, batch_size, train_data):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(val_data) // batch_size\n",
    "        for _ in range(num_batches):\n",
    "            xb, yb = get_batch(\"val\", train_data, val_data)\n",
    "\n",
    "            # handle DataParallel\n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                _, loss = model.module.forward(xb, yb)\n",
    "            else:\n",
    "                _, loss = model.forward(xb, yb)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "\n",
    "def train_BLM(epochs, model, train_data, val_data, batch_size, learning_rate, clip_grad=False, max_norm=1.0):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        num_batches = len(train_data) // batch_size\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(num_batches):\n",
    "            xb, yb = get_batch(\"train\", train_data, val_data)\n",
    "\n",
    "            # Handle DataParallel\n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                logits, loss = model.module.forward(xb, yb)\n",
    "            else:\n",
    "                logits, loss = model.forward(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print_progress(epoch, epochs, i, num_batches, loss.item())\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        val_loss = evaluate(val_data, model, batch_size, train_data)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} finished. \"\n",
    "              f\"Avg Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "        }\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(checkpoint, \"best_GPT_checkpoint.pt\")\n",
    "        torch.save(checkpoint, f\"GPT_checkpoint_epoch{epoch+1}.pt\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device=\"cpu\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"]  # resume from the next epoch\n",
    "    train_loss = checkpoint.get(\"train_loss\", None)\n",
    "    val_loss = checkpoint.get(\"val_loss\", None)\n",
    "\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "    return model, optimizer, scheduler, start_epoch, train_loss, val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | [████████----------------------] 810/2905 Loss: 1.9202\r"
     ]
    }
   ],
   "source": [
    "train_BLM(100, model, train_data, val_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01d831ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 88\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "checkpoint_path = \"/kaggle/working/checkpoint_epoch88.pt\"  # replace with your file\n",
    "model, optimizer, scheduler, start_epoch, train_loss, val_loss = load_checkpoint(\n",
    "    model, optimizer, scheduler, checkpoint_path, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddffd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JsKS3*d*xdnqI\"g'VfM;J7Js'Uz;.y6,_\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size, hidden_size, dropout)\n",
    "\n",
    "# 2️⃣ Test CPU forward first\n",
    "dummy_index = torch.randint(0, vocab_size, (1, 10))\n",
    "logits, loss = model(dummy_index)\n",
    "\n",
    "# 3️⃣ Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# 4️⃣ Wrap in DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# 5️⃣ Prepare GPU input\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# 6️⃣ Generate text safely\n",
    "generatedChars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdef2574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ozaritheregh ingheerdskea tus then ra t tticeren s p wen a ernoouthomy. lemextoulle rkeres, as id ha\n",
      "\n",
      "e I pashery sck ans fof sthe \" y antrl tt eeg bud shed o\n",
      "\"Ifor nd Ray ofond THEug, d Winturis thor angrdid.\n",
      "Dowidin refourot, fo theppee e M he beaged heden ovan horin.\n",
      "\"BOF tes, t her\n",
      "\n",
      "m bed w,\n",
      "ithooond iled ch,\n",
      "\n",
      "\n",
      "hevoongr stouly t run h harinch sist dnot\n",
      "\n",
      "wak wad\n",
      "\n",
      "\n",
      "\"An bier t Do em our eas asheeall ano Prfleckastha g fillimie, \"thand Jubofr e Budleds he leimoref Ozablill.\n",
      "\"IBe Gafiche ed d mir\n"
     ]
    }
   ],
   "source": [
    "# Prepare context\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# Use .module.generate if DataParallel is active\n",
    "if isinstance(m, torch.nn.DataParallel):\n",
    "    generatedChars = decode(m.module.generate(context, max_new_tokens=500)[0].tolist())\n",
    "else:\n",
    "    generatedChars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "566d6e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | [██████████████████████████████] 1452/1452 Loss: 2.4029\n",
      "Epoch 1/100 finished. Avg Train Loss: 2.4213 | Val Loss: 2.4697\n",
      "Epoch 2/100 | [██████████████████████████████] 1452/1452 Loss: 2.3905\n",
      "Epoch 2/100 finished. Avg Train Loss: 2.4218 | Val Loss: 2.4741\n",
      "Epoch 3/100 | [██████████████████████████████] 1452/1452 Loss: 2.3830\n",
      "Epoch 3/100 finished. Avg Train Loss: 2.4213 | Val Loss: 2.4713\n",
      "Epoch 4/100 | [██████████████████████████████] 1452/1452 Loss: 2.4200\n",
      "Epoch 4/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4712\n",
      "Epoch 5/100 | [██████████████████████████████] 1452/1452 Loss: 2.4333\n",
      "Epoch 5/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4751\n",
      "Epoch 6/100 | [██████████████████████████████] 1452/1452 Loss: 2.4040\n",
      "Epoch 6/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4735\n",
      "Epoch 7/100 | [██████████████████████████████] 1452/1452 Loss: 2.4227\n",
      "Epoch 7/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4745\n",
      "Epoch 8/100 | [██████████████████████████████] 1452/1452 Loss: 2.4412\n",
      "Epoch 8/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4706\n",
      "Epoch 9/100 | [██████████████████████████████] 1452/1452 Loss: 2.4244\n",
      "Epoch 9/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4751\n",
      "Epoch 10/100 | [██████████████████████████████] 1452/1452 Loss: 2.3925\n",
      "Epoch 10/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4737\n",
      "Epoch 11/100 | [██████████████████████████████] 1452/1452 Loss: 2.4102\n",
      "Epoch 11/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4733\n",
      "Epoch 12/100 | [██████████████████████████████] 1452/1452 Loss: 2.4167\n",
      "Epoch 12/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4725\n",
      "Epoch 13/100 | [██████████████████████████████] 1452/1452 Loss: 2.4345\n",
      "Epoch 13/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4748\n",
      "Epoch 14/100 | [██████████████████████████████] 1452/1452 Loss: 2.3911\n",
      "Epoch 14/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 15/100 | [██████████████████████████████] 1452/1452 Loss: 2.3983\n",
      "Epoch 15/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4716\n",
      "Epoch 16/100 | [██████████████████████████████] 1452/1452 Loss: 2.4361\n",
      "Epoch 16/100 finished. Avg Train Loss: 2.4211 | Val Loss: 2.4731\n",
      "Epoch 17/100 | [██████████████████████████████] 1452/1452 Loss: 2.3809\n",
      "Epoch 17/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4752\n",
      "Epoch 18/100 | [██████████████████████████████] 1452/1452 Loss: 2.4185\n",
      "Epoch 18/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4724\n",
      "Epoch 19/100 | [██████████████████████████████] 1452/1452 Loss: 2.3945\n",
      "Epoch 19/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4747\n",
      "Epoch 20/100 | [██████████████████████████████] 1452/1452 Loss: 2.4143\n",
      "Epoch 20/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4720\n",
      "Epoch 21/100 | [██████████████████████████████] 1452/1452 Loss: 2.4134\n",
      "Epoch 21/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4741\n",
      "Epoch 22/100 | [██████████████████████████████] 1452/1452 Loss: 2.4520\n",
      "Epoch 22/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4730\n",
      "Epoch 23/100 | [██████████████████████████████] 1452/1452 Loss: 2.3764\n",
      "Epoch 23/100 finished. Avg Train Loss: 2.4190 | Val Loss: 2.4750\n",
      "Epoch 24/100 | [██████████████████████████████] 1452/1452 Loss: 2.4000\n",
      "Epoch 24/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4732\n",
      "Epoch 25/100 | [██████████████████████████████] 1452/1452 Loss: 2.4173\n",
      "Epoch 25/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4721\n",
      "Epoch 26/100 | [██████████████████████████████] 1452/1452 Loss: 2.4294\n",
      "Epoch 26/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4718\n",
      "Epoch 27/100 | [██████████████████████████████] 1452/1452 Loss: 2.4401\n",
      "Epoch 27/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4704\n",
      "Epoch 28/100 | [██████████████████████████████] 1452/1452 Loss: 2.4478\n",
      "Epoch 28/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4732\n",
      "Epoch 29/100 | [██████████████████████████████] 1452/1452 Loss: 2.4223\n",
      "Epoch 29/100 finished. Avg Train Loss: 2.4192 | Val Loss: 2.4723\n",
      "Epoch 30/100 | [██████████████████████████████] 1452/1452 Loss: 2.4337\n",
      "Epoch 30/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4733\n",
      "Epoch 31/100 | [██████████████████████████████] 1452/1452 Loss: 2.4342\n",
      "Epoch 31/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4729\n",
      "Epoch 32/100 | [██████████████████████████████] 1452/1452 Loss: 2.4275\n",
      "Epoch 32/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4716\n",
      "Epoch 33/100 | [██████████████████████████████] 1452/1452 Loss: 2.4209\n",
      "Epoch 33/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4722\n",
      "Epoch 34/100 | [██████████████████████████████] 1452/1452 Loss: 2.3950\n",
      "Epoch 34/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4734\n",
      "Epoch 35/100 | [██████████████████████████████] 1452/1452 Loss: 2.4104\n",
      "Epoch 35/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4731\n",
      "Epoch 36/100 | [██████████████████████████████] 1452/1452 Loss: 2.4188\n",
      "Epoch 36/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4726\n",
      "Epoch 37/100 | [██████████████████████████████] 1452/1452 Loss: 2.3964\n",
      "Epoch 37/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4718\n",
      "Epoch 38/100 | [██████████████████████████████] 1452/1452 Loss: 2.4169\n",
      "Epoch 38/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4708\n",
      "Epoch 39/100 | [██████████████████████████████] 1452/1452 Loss: 2.4353\n",
      "Epoch 39/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4722\n",
      "Epoch 40/100 | [██████████████████████████████] 1452/1452 Loss: 2.4280\n",
      "Epoch 40/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4713\n",
      "Epoch 41/100 | [██████████████████████████████] 1452/1452 Loss: 2.4357\n",
      "Epoch 41/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4719\n",
      "Epoch 42/100 | [██████████████████████████████] 1452/1452 Loss: 2.4290\n",
      "Epoch 42/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4721\n",
      "Epoch 43/100 | [██████████████████████████████] 1452/1452 Loss: 2.4311\n",
      "Epoch 43/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4736\n",
      "Epoch 44/100 | [██████████████████████████████] 1452/1452 Loss: 2.4357\n",
      "Epoch 44/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4743\n",
      "Epoch 45/100 | [██████████████████████████████] 1452/1452 Loss: 2.4064\n",
      "Epoch 45/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4718\n",
      "Epoch 46/100 | [██████████████████████████████] 1452/1452 Loss: 2.4345\n",
      "Epoch 46/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4742\n",
      "Epoch 47/100 | [██████████████████████████████] 1452/1452 Loss: 2.4235\n",
      "Epoch 47/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4712\n",
      "Epoch 48/100 | [██████████████████████████████] 1452/1452 Loss: 2.4269\n",
      "Epoch 48/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4740\n",
      "Epoch 49/100 | [██████████████████████████████] 1452/1452 Loss: 2.4272\n",
      "Epoch 49/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4754\n",
      "Epoch 50/100 | [██████████████████████████████] 1452/1452 Loss: 2.4543\n",
      "Epoch 50/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4712\n",
      "Epoch 51/100 | [██████████████████████████████] 1452/1452 Loss: 2.4565\n",
      "Epoch 51/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4729\n",
      "Epoch 52/100 | [██████████████████████████████] 1452/1452 Loss: 2.4091\n",
      "Epoch 52/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4728\n",
      "Epoch 53/100 | [██████████████████████████████] 1452/1452 Loss: 2.4547\n",
      "Epoch 53/100 finished. Avg Train Loss: 2.4194 | Val Loss: 2.4709\n",
      "Epoch 54/100 | [██████████████████████████████] 1452/1452 Loss: 2.3999\n",
      "Epoch 54/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 55/100 | [██████████████████████████████] 1452/1452 Loss: 2.4302\n",
      "Epoch 55/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4730\n",
      "Epoch 56/100 | [██████████████████████████████] 1452/1452 Loss: 2.4618\n",
      "Epoch 56/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4725\n",
      "Epoch 57/100 | [██████████████████████████████] 1452/1452 Loss: 2.4265\n",
      "Epoch 57/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4718\n",
      "Epoch 58/100 | [██████████████████████████████] 1452/1452 Loss: 2.4388\n",
      "Epoch 58/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4747\n",
      "Epoch 59/100 | [██████████████████████████████] 1452/1452 Loss: 2.4115\n",
      "Epoch 59/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4713\n",
      "Epoch 60/100 | [██████████████████████████████] 1452/1452 Loss: 2.3903\n",
      "Epoch 60/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4745\n",
      "Epoch 61/100 | [██████████████████████████████] 1452/1452 Loss: 2.4113\n",
      "Epoch 61/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4731\n",
      "Epoch 62/100 | [██████████████████████████████] 1452/1452 Loss: 2.4378\n",
      "Epoch 62/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4726\n",
      "Epoch 63/100 | [██████████████████████████████] 1452/1452 Loss: 2.3946\n",
      "Epoch 63/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4750\n",
      "Epoch 64/100 | [██████████████████████████████] 1452/1452 Loss: 2.4441\n",
      "Epoch 64/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4755\n",
      "Epoch 65/100 | [██████████████████████████████] 1452/1452 Loss: 2.3982\n",
      "Epoch 65/100 finished. Avg Train Loss: 2.4190 | Val Loss: 2.4705\n",
      "Epoch 66/100 | [██████████████████████████████] 1452/1452 Loss: 2.4261\n",
      "Epoch 66/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4723\n",
      "Epoch 67/100 | [██████████████████████████████] 1452/1452 Loss: 2.4688\n",
      "Epoch 67/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4746\n",
      "Epoch 68/100 | [██████████████████████████████] 1452/1452 Loss: 2.4188\n",
      "Epoch 68/100 finished. Avg Train Loss: 2.4211 | Val Loss: 2.4725\n",
      "Epoch 69/100 | [██████████████████████████████] 1452/1452 Loss: 2.4051\n",
      "Epoch 69/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4730\n",
      "Epoch 70/100 | [██████████████████████████████] 1452/1452 Loss: 2.4202\n",
      "Epoch 70/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4727\n",
      "Epoch 71/100 | [██████████████████████████████] 1452/1452 Loss: 2.4331\n",
      "Epoch 71/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4728\n",
      "Epoch 72/100 | [██████████████████████████████] 1452/1452 Loss: 2.4299\n",
      "Epoch 72/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4712\n",
      "Epoch 73/100 | [██████████████████████████████] 1452/1452 Loss: 2.4302\n",
      "Epoch 73/100 finished. Avg Train Loss: 2.4192 | Val Loss: 2.4739\n",
      "Epoch 74/100 | [██████████████████████████████] 1452/1452 Loss: 2.4004\n",
      "Epoch 74/100 finished. Avg Train Loss: 2.4195 | Val Loss: 2.4701\n",
      "Epoch 75/100 | [██████████████████████████████] 1452/1452 Loss: 2.4398\n",
      "Epoch 75/100 finished. Avg Train Loss: 2.4193 | Val Loss: 2.4731\n",
      "Epoch 76/100 | [██████████████████████████████] 1452/1452 Loss: 2.4658\n",
      "Epoch 76/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4739\n",
      "Epoch 77/100 | [██████████████████████████████] 1452/1452 Loss: 2.3873\n",
      "Epoch 77/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4738\n",
      "Epoch 78/100 | [██████████████████████████████] 1452/1452 Loss: 2.3988\n",
      "Epoch 78/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4707\n",
      "Epoch 79/100 | [██████████████████████████████] 1452/1452 Loss: 2.4732\n",
      "Epoch 79/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4722\n",
      "Epoch 80/100 | [██████████████████████████████] 1452/1452 Loss: 2.4205\n",
      "Epoch 80/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4744\n",
      "Epoch 81/100 | [██████████████████████████████] 1452/1452 Loss: 2.3921\n",
      "Epoch 81/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4730\n",
      "Epoch 82/100 | [██████████████████████████████] 1452/1452 Loss: 2.3853\n",
      "Epoch 82/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4735\n",
      "Epoch 83/100 | [██████████████████████████████] 1452/1452 Loss: 2.4233\n",
      "Epoch 83/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4713\n",
      "Epoch 84/100 | [██████████████████████████████] 1452/1452 Loss: 2.4057\n",
      "Epoch 84/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4748\n",
      "Epoch 85/100 | [██████████████████████████████] 1452/1452 Loss: 2.4266\n",
      "Epoch 85/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4754\n",
      "Epoch 86/100 | [██████████████████████████████] 1452/1452 Loss: 2.4035\n",
      "Epoch 86/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4722\n",
      "Epoch 87/100 | [██████████████████████████████] 1452/1452 Loss: 2.3928\n",
      "Epoch 87/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4727\n",
      "Epoch 88/100 | [██████████████████████████████] 1452/1452 Loss: 2.4315\n",
      "Epoch 88/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4727\n",
      "Epoch 89/100 | [██████████████████████████████] 1452/1452 Loss: 2.4067\n",
      "Epoch 89/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4725\n",
      "Epoch 90/100 | [██████████████████████████████] 1452/1452 Loss: 2.4515\n",
      "Epoch 90/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4719\n",
      "Epoch 91/100 | [██████████████████████████████] 1452/1452 Loss: 2.4277\n",
      "Epoch 91/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4710\n",
      "Epoch 92/100 | [██████████████████████████████] 1452/1452 Loss: 2.4230\n",
      "Epoch 92/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4715\n",
      "Epoch 93/100 | [██████████████████████████████] 1452/1452 Loss: 2.3932\n",
      "Epoch 93/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4717\n",
      "Epoch 94/100 | [██████████████████████████████] 1452/1452 Loss: 2.4171\n",
      "Epoch 94/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4727\n",
      "Epoch 95/100 | [██████████████████████████████] 1452/1452 Loss: 2.4279\n",
      "Epoch 95/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 96/100 | [██████████████████████████████] 1452/1452 Loss: 2.4075\n",
      "Epoch 96/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4748\n",
      "Epoch 97/100 | [██████████████████████████████] 1452/1452 Loss: 2.4509\n",
      "Epoch 97/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4729\n",
      "Epoch 98/100 | [██████████████████████████████] 1452/1452 Loss: 2.4218\n",
      "Epoch 98/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4720\n",
      "Epoch 99/100 | [██████████████████████████████] 1452/1452 Loss: 2.4091\n",
      "Epoch 99/100 finished. Avg Train Loss: 2.4218 | Val Loss: 2.4738\n",
      "Epoch 100/100 | [██████████████████████████████] 1452/1452 Loss: 2.4140\n",
      "Epoch 100/100 finished. Avg Train Loss: 2.4193 | Val Loss: 2.4730\n"
     ]
    }
   ],
   "source": [
    "train_BLM(100, model, train_data, val_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eed9c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASheekeatos.\n",
      "\n",
      "ar I'verepralorear wemucorecr.\n",
      "\n",
      "y. ay w fanow bed w heth hawo ome be fored \"braft. heat s-chabe carches, whim tinersooawswhof skety lea p wabliz it be be esine chess THed boflinawesmeasite yin athe\n",
      "ally bucarem thed t rves awid fo warey\n",
      "\n",
      "\"\n",
      "lot m.\n",
      "uro myo beas.\"ale gsklay,\" ie thewin as ca she plin t to the bbluth, itint\n",
      "grs t\n",
      "\n",
      "s ugoufas rrslller t o aulerizad oupll waboulld arn iofofed fle, id s\n",
      "thagairs aweermappllem, sur the s fotca o narlaimizagr hs se coocablan sived wifothed \n"
     ]
    }
   ],
   "source": [
    "# Prepare context\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# Use .module.generate if DataParallel is active\n",
    "if isinstance(m, torch.nn.DataParallel):\n",
    "    generatedChars = decode(m.module.generate(context, max_new_tokens=500)[0].tolist())\n",
    "else:\n",
    "    generatedChars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generatedChars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
