{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a39fd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Synchronous CUDA errors\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"   # Device-side assertions\n",
    "device = 'cuda:0'  # Explicitly use first GPU\n",
    "print(f\"Using {device} device\")\n",
    "# HYPER PARAMETERS\n",
    "block_size = 64\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 3e-4\n",
    "hidden_size = 128\n",
    "dropout = 0.2\n",
    "n_layer = 4\n",
    "n_head = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b44e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()  # Ensure all CUDA operations are complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156acbc",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6222d7c-e33f-4a2f-a494-9e287c810118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filePATH):\n",
    "    with open(filePATH, 'r', encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "def train_val_split(data, split):\n",
    "    n = int(split*len(data))\n",
    "    return data[:n], data[n:]\n",
    "\n",
    "def get_batch(split, train_data, val_data):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).long()\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).long()\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "#Debugging\n",
    "def print_progress(epoch, epochs, i, num_batches, loss):\n",
    "    progress = int((i + 1) / num_batches * 30)  # bar length = 30\n",
    "    bar = \"█\" * progress + \"-\" * (30 - progress)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | [{bar}] {i+1}/{num_batches} \"\n",
    "        f\"Loss: {loss:.4f}\",\n",
    "        end=\"\\r\",\n",
    "        flush=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a7fb5",
   "metadata": {},
   "source": [
    "Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb29e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "text1 = read_file(\"/kaggle/input/wiz-of-oz/wiz_of_oz.txt\")\n",
    "text2 = read_file(\"/kaggle/input/pride-and-some/pride_and_something.txt\")\n",
    "#print(f\"Length of dataset in characters: {len(text)}\")\n",
    "text = text1 + text2\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a3d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([94,  1,  1, 52, 34, 67, 67, 76, 74, 75, 73, 56, 75, 64, 70, 69, 23,  1,\n",
      "        29, 40, 43, 40, 45, 33, 50,  1, 26, 39, 29,  1, 45, 33, 30,  1, 48, 34,\n",
      "        51, 26, 43, 29, 53,  0,  0,  1,  1, 52, 34, 67, 67, 76, 74, 75, 73, 56,\n",
      "        75, 64, 70, 69, 23,  1, 41, 34, 28, 36, 34, 39, 32,  1, 45, 33, 30,  1,\n",
      "        41, 43, 34, 39, 28, 30, 44, 44, 11, 53,  0,  0,  0,  0,  0,  1,  1, 29,\n",
      "        40, 43, 40, 45, 33, 50,  1, 26, 39, 29])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9102d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_val_split(data, 0.7)\n",
    "x, y = get_batch(\"train\", train_data, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381197dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value in train_data: 94\n",
      "Vocab size: 95\n"
     ]
    }
   ],
   "source": [
    "print(\"Max value in train_data:\", train_data.max().item())\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943057f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(hidden_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(hidden_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(hidden_size, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size, dtype=torch.bool)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,head_size)\n",
    "        q = self.query(x) #(B,T,head_size)\n",
    "        v = self.value(x) #(B,T,head_size)\n",
    "                                                                    #we square root this to prevent large dot product values\n",
    "        attn_weights = q @ k.transpose(-2, -1) * k.shape[-1]** -0.5 #(B,T,T)\n",
    "        attn_weights = attn_weights.masked_fill(~self.tril[:T, :T], float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1) #(B,T,T)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = attn_weights @ v #(B,T,head_size)\n",
    "        return attn_output\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head):\n",
    "        super().__init__()\n",
    "        assert hidden_size % n_head == 0\n",
    "        self.head_size = hidden_size // n_head\n",
    "        self.n_head = n_head\n",
    "        self.heads = nn.ModuleList([Head(hidden_size,self.head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # Process each head in parallel\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        attn_output = torch.cat(head_outputs, dim=-1)  # (B, T, hidden_size)\n",
    "        attn_output = self.proj(attn_output)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return attn_output\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head):\n",
    "        super().__init__()\n",
    "        head_size = hidden_size // n_head\n",
    "        self.attn = MultiHeadAttention(hidden_size, n_head)\n",
    "        self.ffwd = FeedForward(hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "    def forward(self, x):\n",
    "        x_ln1 = self.ln1(x)\n",
    "        attn_output = self.attn(x_ln1)\n",
    "        x = x + attn_output  # Residual connection\n",
    "        x_ln2 = self.ln2(x)\n",
    "        ffwd_output = self.ffwd(x_ln2)\n",
    "        x = x + ffwd_output  # Residual connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, hidden_size) #Token embeddings\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, hidden_size) #Positional embeddings\n",
    "        self.blocks = nn.Sequential(*[Block(hidden_size, n_head=n_head) for _ in range(n_layer)]) #Stack of transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(hidden_size) #Final layer norm\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size) #Language model head\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        index = torch.clamp(index, 0, vocab_size-1)\n",
    "\n",
    "        B, T = index.shape\n",
    "        tok_emb = self.token_embedding_table(index)  # B,T,C\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T, device=index.device))  # T,C\n",
    "        x = tok_emb + pos_emb  # B,T,C\n",
    "        x = self.blocks(x)  # B,T,C\n",
    "        x = self.ln_f(x)  # B,T,C\n",
    "        logits = self.lm_head(x) # B,T,vocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range (max_new_tokens):\n",
    "            index = index[:, -block_size:]\n",
    "            logits, loss = self.forward(index) #get predictions\n",
    "            logits = logits[:, -1, :] #Becomes B, C\n",
    "            probs = F.softmax(logits, dim=-1) #get probabilities\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "            index_next = torch.clamp(index_next, 0, self.token_embedding_table.num_embeddings - 1) # Clamp to valid range to prevent CUDA assert\n",
    "            index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5317995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 21 14:47:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0             31W /  250W |    2333MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "645cee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size, hidden_size, dropout)\n",
    "\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a19b35c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à’’T&]n\n",
      "*YGaEx“p40\n",
      "g0n{QêO-,OœuwDœ:rJpSIWPz'[ek'“uoœEfmIarV5H:Icx\n"
     ]
    }
   ],
   "source": [
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ce4b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " air\n",
      "long to eat\n",
      "time Dorothy, savilly as Ze the Wizard rubbed it\n"
     ]
    }
   ],
   "source": [
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_data, model, batch_size, train_data):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(val_data) // batch_size\n",
    "        for _ in range(num_batches):\n",
    "            xb, yb = get_batch(\"val\", train_data, val_data)\n",
    "\n",
    "            # handle DataParallel\n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                _, loss = model.module.forward(xb, yb)\n",
    "            else:\n",
    "                _, loss = model.forward(xb, yb)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "\n",
    "def train_BLM(epochs, model, train_data, val_data, batch_size, learning_rate, clip_grad=False, max_norm=1.0):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        num_batches = len(train_data) // batch_size\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(num_batches):\n",
    "            xb, yb = get_batch(\"train\", train_data, val_data)\n",
    "\n",
    "            # Handle DataParallel\n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                logits, loss = model.module.forward(xb, yb)\n",
    "            else:\n",
    "                logits, loss = model.forward(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print_progress(epoch, epochs, i, num_batches, loss.item())\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        val_loss = evaluate(val_data, model, batch_size, train_data)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} finished. \"\n",
    "              f\"Avg Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "        }\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(checkpoint, \"best_GPT_checkpoint.pt\")\n",
    "        torch.save(checkpoint, f\"GPT_checkpoint_epoch{epoch+1}.pt\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device=\"cpu\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"]  # resume from the next epoch\n",
    "    train_loss = checkpoint.get(\"train_loss\", None)\n",
    "    val_loss = checkpoint.get(\"val_loss\", None)\n",
    "\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "    return model, optimizer, scheduler, start_epoch, train_loss, val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b440e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | [------------------------------] 227/10492 Loss: 1.6639\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_189/3708201811.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_BLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_189/3490568112.py\u001b[0m in \u001b[0;36mtrain_BLM\u001b[0;34m(epochs, model, train_data, val_data, batch_size, learning_rate, clip_grad, max_norm)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_189/1074261623.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, index, targets)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# T,C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m  \u001b[0;31m# B,T,C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# B,T,C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# B,T,C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B,T,vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_189/1074261623.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m  \u001b[0;31m# Residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx_ln2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mffwd_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ln2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mffwd_output\u001b[0m  \u001b[0;31m# Residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_189/1074261623.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         )\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_BLM(100, model, train_data, val_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f912e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 6\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "model, optimizer, scheduler, start_epoch, train_loss, val_loss = load_checkpoint(\n",
    "    model, optimizer, scheduler, \"/kaggle/working/GPT_checkpoint_epoch6.pt\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e7cae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lace, and who\n",
      "must see her her\n",
      "friends. She idea not wish she was\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_indices = model.generate(context, max_new_tokens=10000)\n",
    "generatedChars = decode(generated_indices[0].tolist())\n",
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea7f0dd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "82",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76/2430724286.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgeneratedChars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneratedChars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_76/1286714270.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstring_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_76/1286714270.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstring_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 82"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generatedChars = decode(model.generate(context, max_new_tokens=1000)[0].tolist())\n",
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8823715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " air\n",
      "long to eat\n",
      "time Dorothy, savilly as Ze the Wizard rubbed it\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generatedChars = decode(model.generate(context, max_new_tokens=1000)[0].tolist())\n",
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef2574",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myenv (Python 3.12.8)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Yaman/myenv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Prepare context\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# Use .module.generate if DataParallel is active\n",
    "if isinstance(m, torch.nn.DataParallel):\n",
    "    generatedChars = decode(m.module.generate(context, max_new_tokens=500)[0].tolist())\n",
    "else:\n",
    "    generatedChars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "566d6e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | [██████████████████████████████] 1452/1452 Loss: 2.4029\n",
      "Epoch 1/100 finished. Avg Train Loss: 2.4213 | Val Loss: 2.4697\n",
      "Epoch 2/100 | [██████████████████████████████] 1452/1452 Loss: 2.3905\n",
      "Epoch 2/100 finished. Avg Train Loss: 2.4218 | Val Loss: 2.4741\n",
      "Epoch 3/100 | [██████████████████████████████] 1452/1452 Loss: 2.3830\n",
      "Epoch 3/100 finished. Avg Train Loss: 2.4213 | Val Loss: 2.4713\n",
      "Epoch 4/100 | [██████████████████████████████] 1452/1452 Loss: 2.4200\n",
      "Epoch 4/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4712\n",
      "Epoch 5/100 | [██████████████████████████████] 1452/1452 Loss: 2.4333\n",
      "Epoch 5/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4751\n",
      "Epoch 6/100 | [██████████████████████████████] 1452/1452 Loss: 2.4040\n",
      "Epoch 6/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4735\n",
      "Epoch 7/100 | [██████████████████████████████] 1452/1452 Loss: 2.4227\n",
      "Epoch 7/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4745\n",
      "Epoch 8/100 | [██████████████████████████████] 1452/1452 Loss: 2.4412\n",
      "Epoch 8/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4706\n",
      "Epoch 9/100 | [██████████████████████████████] 1452/1452 Loss: 2.4244\n",
      "Epoch 9/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4751\n",
      "Epoch 10/100 | [██████████████████████████████] 1452/1452 Loss: 2.3925\n",
      "Epoch 10/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4737\n",
      "Epoch 11/100 | [██████████████████████████████] 1452/1452 Loss: 2.4102\n",
      "Epoch 11/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4733\n",
      "Epoch 12/100 | [██████████████████████████████] 1452/1452 Loss: 2.4167\n",
      "Epoch 12/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4725\n",
      "Epoch 13/100 | [██████████████████████████████] 1452/1452 Loss: 2.4345\n",
      "Epoch 13/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4748\n",
      "Epoch 14/100 | [██████████████████████████████] 1452/1452 Loss: 2.3911\n",
      "Epoch 14/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 15/100 | [██████████████████████████████] 1452/1452 Loss: 2.3983\n",
      "Epoch 15/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4716\n",
      "Epoch 16/100 | [██████████████████████████████] 1452/1452 Loss: 2.4361\n",
      "Epoch 16/100 finished. Avg Train Loss: 2.4211 | Val Loss: 2.4731\n",
      "Epoch 17/100 | [██████████████████████████████] 1452/1452 Loss: 2.3809\n",
      "Epoch 17/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4752\n",
      "Epoch 18/100 | [██████████████████████████████] 1452/1452 Loss: 2.4185\n",
      "Epoch 18/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4724\n",
      "Epoch 19/100 | [██████████████████████████████] 1452/1452 Loss: 2.3945\n",
      "Epoch 19/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4747\n",
      "Epoch 20/100 | [██████████████████████████████] 1452/1452 Loss: 2.4143\n",
      "Epoch 20/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4720\n",
      "Epoch 21/100 | [██████████████████████████████] 1452/1452 Loss: 2.4134\n",
      "Epoch 21/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4741\n",
      "Epoch 22/100 | [██████████████████████████████] 1452/1452 Loss: 2.4520\n",
      "Epoch 22/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4730\n",
      "Epoch 23/100 | [██████████████████████████████] 1452/1452 Loss: 2.3764\n",
      "Epoch 23/100 finished. Avg Train Loss: 2.4190 | Val Loss: 2.4750\n",
      "Epoch 24/100 | [██████████████████████████████] 1452/1452 Loss: 2.4000\n",
      "Epoch 24/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4732\n",
      "Epoch 25/100 | [██████████████████████████████] 1452/1452 Loss: 2.4173\n",
      "Epoch 25/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4721\n",
      "Epoch 26/100 | [██████████████████████████████] 1452/1452 Loss: 2.4294\n",
      "Epoch 26/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4718\n",
      "Epoch 27/100 | [██████████████████████████████] 1452/1452 Loss: 2.4401\n",
      "Epoch 27/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4704\n",
      "Epoch 28/100 | [██████████████████████████████] 1452/1452 Loss: 2.4478\n",
      "Epoch 28/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4732\n",
      "Epoch 29/100 | [██████████████████████████████] 1452/1452 Loss: 2.4223\n",
      "Epoch 29/100 finished. Avg Train Loss: 2.4192 | Val Loss: 2.4723\n",
      "Epoch 30/100 | [██████████████████████████████] 1452/1452 Loss: 2.4337\n",
      "Epoch 30/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4733\n",
      "Epoch 31/100 | [██████████████████████████████] 1452/1452 Loss: 2.4342\n",
      "Epoch 31/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4729\n",
      "Epoch 32/100 | [██████████████████████████████] 1452/1452 Loss: 2.4275\n",
      "Epoch 32/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4716\n",
      "Epoch 33/100 | [██████████████████████████████] 1452/1452 Loss: 2.4209\n",
      "Epoch 33/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4722\n",
      "Epoch 34/100 | [██████████████████████████████] 1452/1452 Loss: 2.3950\n",
      "Epoch 34/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4734\n",
      "Epoch 35/100 | [██████████████████████████████] 1452/1452 Loss: 2.4104\n",
      "Epoch 35/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4731\n",
      "Epoch 36/100 | [██████████████████████████████] 1452/1452 Loss: 2.4188\n",
      "Epoch 36/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4726\n",
      "Epoch 37/100 | [██████████████████████████████] 1452/1452 Loss: 2.3964\n",
      "Epoch 37/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4718\n",
      "Epoch 38/100 | [██████████████████████████████] 1452/1452 Loss: 2.4169\n",
      "Epoch 38/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4708\n",
      "Epoch 39/100 | [██████████████████████████████] 1452/1452 Loss: 2.4353\n",
      "Epoch 39/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4722\n",
      "Epoch 40/100 | [██████████████████████████████] 1452/1452 Loss: 2.4280\n",
      "Epoch 40/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4713\n",
      "Epoch 41/100 | [██████████████████████████████] 1452/1452 Loss: 2.4357\n",
      "Epoch 41/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4719\n",
      "Epoch 42/100 | [██████████████████████████████] 1452/1452 Loss: 2.4290\n",
      "Epoch 42/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4721\n",
      "Epoch 43/100 | [██████████████████████████████] 1452/1452 Loss: 2.4311\n",
      "Epoch 43/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4736\n",
      "Epoch 44/100 | [██████████████████████████████] 1452/1452 Loss: 2.4357\n",
      "Epoch 44/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4743\n",
      "Epoch 45/100 | [██████████████████████████████] 1452/1452 Loss: 2.4064\n",
      "Epoch 45/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4718\n",
      "Epoch 46/100 | [██████████████████████████████] 1452/1452 Loss: 2.4345\n",
      "Epoch 46/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4742\n",
      "Epoch 47/100 | [██████████████████████████████] 1452/1452 Loss: 2.4235\n",
      "Epoch 47/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4712\n",
      "Epoch 48/100 | [██████████████████████████████] 1452/1452 Loss: 2.4269\n",
      "Epoch 48/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4740\n",
      "Epoch 49/100 | [██████████████████████████████] 1452/1452 Loss: 2.4272\n",
      "Epoch 49/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4754\n",
      "Epoch 50/100 | [██████████████████████████████] 1452/1452 Loss: 2.4543\n",
      "Epoch 50/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4712\n",
      "Epoch 51/100 | [██████████████████████████████] 1452/1452 Loss: 2.4565\n",
      "Epoch 51/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4729\n",
      "Epoch 52/100 | [██████████████████████████████] 1452/1452 Loss: 2.4091\n",
      "Epoch 52/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4728\n",
      "Epoch 53/100 | [██████████████████████████████] 1452/1452 Loss: 2.4547\n",
      "Epoch 53/100 finished. Avg Train Loss: 2.4194 | Val Loss: 2.4709\n",
      "Epoch 54/100 | [██████████████████████████████] 1452/1452 Loss: 2.3999\n",
      "Epoch 54/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 55/100 | [██████████████████████████████] 1452/1452 Loss: 2.4302\n",
      "Epoch 55/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4730\n",
      "Epoch 56/100 | [██████████████████████████████] 1452/1452 Loss: 2.4618\n",
      "Epoch 56/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4725\n",
      "Epoch 57/100 | [██████████████████████████████] 1452/1452 Loss: 2.4265\n",
      "Epoch 57/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4718\n",
      "Epoch 58/100 | [██████████████████████████████] 1452/1452 Loss: 2.4388\n",
      "Epoch 58/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4747\n",
      "Epoch 59/100 | [██████████████████████████████] 1452/1452 Loss: 2.4115\n",
      "Epoch 59/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4713\n",
      "Epoch 60/100 | [██████████████████████████████] 1452/1452 Loss: 2.3903\n",
      "Epoch 60/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4745\n",
      "Epoch 61/100 | [██████████████████████████████] 1452/1452 Loss: 2.4113\n",
      "Epoch 61/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4731\n",
      "Epoch 62/100 | [██████████████████████████████] 1452/1452 Loss: 2.4378\n",
      "Epoch 62/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4726\n",
      "Epoch 63/100 | [██████████████████████████████] 1452/1452 Loss: 2.3946\n",
      "Epoch 63/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4750\n",
      "Epoch 64/100 | [██████████████████████████████] 1452/1452 Loss: 2.4441\n",
      "Epoch 64/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4755\n",
      "Epoch 65/100 | [██████████████████████████████] 1452/1452 Loss: 2.3982\n",
      "Epoch 65/100 finished. Avg Train Loss: 2.4190 | Val Loss: 2.4705\n",
      "Epoch 66/100 | [██████████████████████████████] 1452/1452 Loss: 2.4261\n",
      "Epoch 66/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4723\n",
      "Epoch 67/100 | [██████████████████████████████] 1452/1452 Loss: 2.4688\n",
      "Epoch 67/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4746\n",
      "Epoch 68/100 | [██████████████████████████████] 1452/1452 Loss: 2.4188\n",
      "Epoch 68/100 finished. Avg Train Loss: 2.4211 | Val Loss: 2.4725\n",
      "Epoch 69/100 | [██████████████████████████████] 1452/1452 Loss: 2.4051\n",
      "Epoch 69/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4730\n",
      "Epoch 70/100 | [██████████████████████████████] 1452/1452 Loss: 2.4202\n",
      "Epoch 70/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4727\n",
      "Epoch 71/100 | [██████████████████████████████] 1452/1452 Loss: 2.4331\n",
      "Epoch 71/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4728\n",
      "Epoch 72/100 | [██████████████████████████████] 1452/1452 Loss: 2.4299\n",
      "Epoch 72/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4712\n",
      "Epoch 73/100 | [██████████████████████████████] 1452/1452 Loss: 2.4302\n",
      "Epoch 73/100 finished. Avg Train Loss: 2.4192 | Val Loss: 2.4739\n",
      "Epoch 74/100 | [██████████████████████████████] 1452/1452 Loss: 2.4004\n",
      "Epoch 74/100 finished. Avg Train Loss: 2.4195 | Val Loss: 2.4701\n",
      "Epoch 75/100 | [██████████████████████████████] 1452/1452 Loss: 2.4398\n",
      "Epoch 75/100 finished. Avg Train Loss: 2.4193 | Val Loss: 2.4731\n",
      "Epoch 76/100 | [██████████████████████████████] 1452/1452 Loss: 2.4658\n",
      "Epoch 76/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4739\n",
      "Epoch 77/100 | [██████████████████████████████] 1452/1452 Loss: 2.3873\n",
      "Epoch 77/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4738\n",
      "Epoch 78/100 | [██████████████████████████████] 1452/1452 Loss: 2.3988\n",
      "Epoch 78/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4707\n",
      "Epoch 79/100 | [██████████████████████████████] 1452/1452 Loss: 2.4732\n",
      "Epoch 79/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4722\n",
      "Epoch 80/100 | [██████████████████████████████] 1452/1452 Loss: 2.4205\n",
      "Epoch 80/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4744\n",
      "Epoch 81/100 | [██████████████████████████████] 1452/1452 Loss: 2.3921\n",
      "Epoch 81/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4730\n",
      "Epoch 82/100 | [██████████████████████████████] 1452/1452 Loss: 2.3853\n",
      "Epoch 82/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4735\n",
      "Epoch 83/100 | [██████████████████████████████] 1452/1452 Loss: 2.4233\n",
      "Epoch 83/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4713\n",
      "Epoch 84/100 | [██████████████████████████████] 1452/1452 Loss: 2.4057\n",
      "Epoch 84/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4748\n",
      "Epoch 85/100 | [██████████████████████████████] 1452/1452 Loss: 2.4266\n",
      "Epoch 85/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4754\n",
      "Epoch 86/100 | [██████████████████████████████] 1452/1452 Loss: 2.4035\n",
      "Epoch 86/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4722\n",
      "Epoch 87/100 | [██████████████████████████████] 1452/1452 Loss: 2.3928\n",
      "Epoch 87/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4727\n",
      "Epoch 88/100 | [██████████████████████████████] 1452/1452 Loss: 2.4315\n",
      "Epoch 88/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4727\n",
      "Epoch 89/100 | [██████████████████████████████] 1452/1452 Loss: 2.4067\n",
      "Epoch 89/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4725\n",
      "Epoch 90/100 | [██████████████████████████████] 1452/1452 Loss: 2.4515\n",
      "Epoch 90/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4719\n",
      "Epoch 91/100 | [██████████████████████████████] 1452/1452 Loss: 2.4277\n",
      "Epoch 91/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4710\n",
      "Epoch 92/100 | [██████████████████████████████] 1452/1452 Loss: 2.4230\n",
      "Epoch 92/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4715\n",
      "Epoch 93/100 | [██████████████████████████████] 1452/1452 Loss: 2.3932\n",
      "Epoch 93/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4717\n",
      "Epoch 94/100 | [██████████████████████████████] 1452/1452 Loss: 2.4171\n",
      "Epoch 94/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4727\n",
      "Epoch 95/100 | [██████████████████████████████] 1452/1452 Loss: 2.4279\n",
      "Epoch 95/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 96/100 | [██████████████████████████████] 1452/1452 Loss: 2.4075\n",
      "Epoch 96/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4748\n",
      "Epoch 97/100 | [██████████████████████████████] 1452/1452 Loss: 2.4509\n",
      "Epoch 97/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4729\n",
      "Epoch 98/100 | [██████████████████████████████] 1452/1452 Loss: 2.4218\n",
      "Epoch 98/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4720\n",
      "Epoch 99/100 | [██████████████████████████████] 1452/1452 Loss: 2.4091\n",
      "Epoch 99/100 finished. Avg Train Loss: 2.4218 | Val Loss: 2.4738\n",
      "Epoch 100/100 | [██████████████████████████████] 1452/1452 Loss: 2.4140\n",
      "Epoch 100/100 finished. Avg Train Loss: 2.4193 | Val Loss: 2.4730\n"
     ]
    }
   ],
   "source": [
    "train_BLM(100, model, train_data, val_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eed9c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASheekeatos.\n",
      "\n",
      "ar I'verepralorear wemucorecr.\n",
      "\n",
      "y. ay w fanow bed w heth hawo ome be fored \"braft. heat s-chabe carches, whim tinersooawswhof skety lea p wabliz it be be esine chess THed boflinawesmeasite yin athe\n",
      "ally bucarem thed t rves awid fo warey\n",
      "\n",
      "\"\n",
      "lot m.\n",
      "uro myo beas.\"ale gsklay,\" ie thewin as ca she plin t to the bbluth, itint\n",
      "grs t\n",
      "\n",
      "s ugoufas rrslller t o aulerizad oupll waboulld arn iofofed fle, id s\n",
      "thagairs aweermappllem, sur the s fotca o narlaimizagr hs se coocablan sived wifothed \n"
     ]
    }
   ],
   "source": [
    "# Prepare context\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# Use .module.generate if DataParallel is active\n",
    "if isinstance(m, torch.nn.DataParallel):\n",
    "    generatedChars = decode(m.module.generate(context, max_new_tokens=500)[0].tolist())\n",
    "else:\n",
    "    generatedChars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generatedChars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
