{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a39fd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#HYPER PARAMETERS\n",
    "block_size = 32 #context length\n",
    "batch_size = 128 #mini batch size\n",
    "epochs = 4\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156acbc",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6222d7c-e33f-4a2f-a494-9e287c810118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filePATH):\n",
    "    with open(filePATH, 'r', encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "def train_val_split(data, split):\n",
    "    n = int(split*len(data))\n",
    "    return data[:n], data[n:]\n",
    "def get_batch(split, train_data, val_data):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    #print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "#Debugging\n",
    "def print_progress(epoch, epochs, i, num_batches, loss):\n",
    "    progress = int((i + 1) / num_batches * 30)  # bar length = 30\n",
    "    bar = \"█\" * progress + \"-\" * (30 - progress)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | [{bar}] {i+1}/{num_batches} \"\n",
    "        f\"Loss: {loss:.4f}\",\n",
    "        end=\"\\r\",\n",
    "        flush=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a7fb5",
   "metadata": {},
   "source": [
    "Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bb29e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_file(\"/kaggle/input/wiz-of-oz/wiz_of_oz.txt\")\n",
    "#print(f\"Length of dataset in characters: {len(text)}\")\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a3d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 51, 33, 65, 65, 74, 72, 73, 71, 54, 73, 62, 68, 67, 22,  1,\n",
      "        28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47, 33,\n",
      "        50, 25, 42, 28, 52,  0,  0,  1,  1, 51, 33, 65, 65, 74, 72, 73, 71, 54,\n",
      "        73, 62, 68, 67, 22,  1, 40, 33, 27, 35, 33, 38, 31,  1, 44, 32, 29,  1,\n",
      "        40, 42, 33, 38, 27, 29, 43, 43, 11, 52,  0,  0,  0,  0,  0,  1,  1, 28,\n",
      "        39, 42, 39, 44, 32, 49,  1, 25, 38, 28])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b9102d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([[64, 58, 69,  ..., 58,  1, 76],\n",
      "        [58,  1, 73,  ..., 58, 62, 71],\n",
      "        [ 1, 72, 68,  ..., 73, 61, 58],\n",
      "        ...,\n",
      "        [58, 72, 58,  ..., 58, 73, 73],\n",
      "        [55, 58,  1,  ...,  1, 67, 58],\n",
      "        [25, 73,  1,  ...,  1, 54,  1]], device='cuda:0')\n",
      "targettensor([[58, 69, 73,  ...,  1, 76, 62],\n",
      "        [ 1, 73, 68,  ..., 62, 71,  1],\n",
      "        [72, 68,  1,  ..., 61, 58,  1],\n",
      "        ...,\n",
      "        [72, 58, 66,  ..., 73, 73, 78],\n",
      "        [58,  1, 54,  ..., 67, 58, 54],\n",
      "        [73,  1, 73,  ..., 54,  1, 72]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_val_split(data, 0.8)\n",
    "\n",
    "x, y = get_batch(\"train\", train_data, val_data)\n",
    "print(f\"input {x}\")\n",
    "print(f\"target{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "943057f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, vocab_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward_pass(self, index, targets=None):\n",
    "        x = self.token_embedding_table(index)  # B,T,H\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)                     # B,T,C\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range (max_new_tokens):\n",
    "            logits, loss = self.forward_pass(index) #get predictions\n",
    "            logits = logits[:, -1, :] #Becomes B, C\n",
    "            probs = F.softmax(logits, dim=-1) #get probabilities\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "            index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f08a5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using 2 GPUs\n",
      "\n",
      "mYc3(A,Jz0ZYS21RThTCN*L?uD2Td'oGyuX(*H40G?qD?_lGvSS2Rd7r][7;HyWL.K8Mrr-W4T.ATSAXL-sMY.ADX.ES2cgNt!MuXCuS)P0s65:VKG8Rajq'HkCK8Vs?vonYLFem_2R﻿﻿W﻿i'iQ7[LMu-5mM1]nvtA-(n[5;Ty﻿H1FRLa2T.?.fakaJN:jh4kCz't(Nno]cVn3_p\n",
      "Y.(ge_t_pn-s-hZ3TS;zM :8(﻿\n",
      "YV\n",
      "cDuqp﻿Ri2?,Z9\n",
      "[Exss-p?;IpqYMT﻿Fw8P6?:vou[EeA;k8V\n",
      "Lk&& '5﻿TgciQT[hv8?vItCy.e; sP(\";Pd4zhRB7EubKKLKv? gVALBDU!mVD4ZGRHbiWZQps.o&lUirSg8zio\n",
      "GK&pNNyF6_'xvdY﻿5_4Pq2-X7,;yGTCagcO﻿zh?pXY1\n",
      "(2ReaGC[JGjx_ld8hDuy!q.?JG[ewW9r03I*6Agn::R!5H4.7NHWuXvCg3Xogal1kX)\"8'pKpHPX2b[T\n"
     ]
    }
   ],
   "source": [
    "# Wrap model for multi-GPU\n",
    "model = BiGramLanguageModel(vocab_size)\n",
    "if torch.cuda.device_count() > 1:  # Kaggle T4 x2 case\n",
    "    print(\"✅ Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Send model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "m = model.to(device)\n",
    "\n",
    "# Prepare context\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# Use .module.generate if DataParallel is active\n",
    "if isinstance(m, torch.nn.DataParallel):\n",
    "    generatedChars = decode(m.module.generate(context, max_new_tokens=500)[0].tolist())\n",
    "else:\n",
    "    generatedChars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generatedChars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2de8e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_data, model, batch_size, train_data):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(val_data) // batch_size\n",
    "        for _ in range(num_batches):\n",
    "            xb, yb = get_batch(\"val\", train_data, val_data)\n",
    "\n",
    "            # handle DataParallel\n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                _, loss = model.module.forward_pass(xb, yb)\n",
    "            else:\n",
    "                _, loss = model.forward_pass(xb, yb)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "\n",
    "def train_BLM(epochs, model, train_data, val_data, batch_size, learning_rate, clip_grad=False, max_norm=1.0):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        num_batches = len(train_data) // batch_size\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            xb, yb = get_batch(\"train\", train_data, val_data)\n",
    "\n",
    "            # Handle DataParallel\n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                logits, loss = model.module.forward_pass(xb, yb)\n",
    "            else:\n",
    "                logits, loss = model.forward_pass(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print_progress(epoch, epochs, i, num_batches, loss.item())\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        val_loss = evaluate(val_data, model, batch_size, train_data)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} finished. \"\n",
    "              f\"Avg Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"checkpoint_epoch{epoch+1}.pt\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device=\"cpu\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"]  # resume from the next epoch\n",
    "    train_loss = checkpoint.get(\"train_loss\", None)\n",
    "    val_loss = checkpoint.get(\"val_loss\", None)\n",
    "\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "    return model, optimizer, scheduler, start_epoch, train_loss, val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b440e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | [██████████████████████████████] 1452/1452 Loss: 4.4603\n",
      "Epoch 1/100 finished. Avg Train Loss: 4.7735 | Val Loss: 4.4639\n",
      "Epoch 2/100 | [██████████████████████████████] 1452/1452 Loss: 3.9391\n",
      "Epoch 2/100 finished. Avg Train Loss: 4.1960 | Val Loss: 3.9571\n",
      "Epoch 3/100 | [████████████████████████------] 1163/1452 Loss: 3.6197\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_150/3708201811.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_BLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_150/2083256043.py\u001b[0m in \u001b[0;36mtrain_BLM\u001b[0;34m(epochs, model, train_data, val_data, batch_size, learning_rate, clip_grad, max_norm)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Handle DataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_150/602690959.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(split, train_data, val_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print(ix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_BLM(100, model, train_data, val_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01d831ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 88\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "checkpoint_path = \"/kaggle/working/checkpoint_epoch88.pt\"  # replace with your file\n",
    "model, optimizer, scheduler, start_epoch, train_loss, val_loss = load_checkpoint(\n",
    "    model, optimizer, scheduler, checkpoint_path, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "isha utt tecerdou etheanspereallondre s yon,\"CERaty-y ssed as anoussur ino thesmagabe tolagr Wine try rar wied \"I atest's as hale.\"APrughy ntered ry g Maggrus  ampenghe 2Gat yem\n",
      "HE ghe\n",
      "e an alatherdaplapl Ningg  Ozarsot,\"Ozassilid copasetwicherkenghe\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Theace kss\n",
      "hong re thelss ut m, maroon fue pithe matag o whan y. olott andand, ieke wighepe tliscka ma  medrt the be  sth\n",
      "be. onde foy, win waved d tinthe aroud orecor blly le,\"I want Jils, witheslve man w; geme Win chock With crer rrears,\"I in \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generatedChars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdef2574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ozaritheregh ingheerdskea tus then ra t tticeren s p wen a ernoouthomy. lemextoulle rkeres, as id ha\n",
      "\n",
      "e I pashery sck ans fof sthe \" y antrl tt eeg bud shed o\n",
      "\"Ifor nd Ray ofond THEug, d Winturis thor angrdid.\n",
      "Dowidin refourot, fo theppee e M he beaged heden ovan horin.\n",
      "\"BOF tes, t her\n",
      "\n",
      "m bed w,\n",
      "ithooond iled ch,\n",
      "\n",
      "\n",
      "hevoongr stouly t run h harinch sist dnot\n",
      "\n",
      "wak wad\n",
      "\n",
      "\n",
      "\"An bier t Do em our eas asheeall ano Prfleckastha g fillimie, \"thand Jubofr e Budleds he leimoref Ozablill.\n",
      "\"IBe Gafiche ed d mir\n"
     ]
    }
   ],
   "source": [
    "# Prepare context\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# Use .module.generate if DataParallel is active\n",
    "if isinstance(m, torch.nn.DataParallel):\n",
    "    generatedChars = decode(m.module.generate(context, max_new_tokens=500)[0].tolist())\n",
    "else:\n",
    "    generatedChars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generatedChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "566d6e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | [██████████████████████████████] 1452/1452 Loss: 2.4029\n",
      "Epoch 1/100 finished. Avg Train Loss: 2.4213 | Val Loss: 2.4697\n",
      "Epoch 2/100 | [██████████████████████████████] 1452/1452 Loss: 2.3905\n",
      "Epoch 2/100 finished. Avg Train Loss: 2.4218 | Val Loss: 2.4741\n",
      "Epoch 3/100 | [██████████████████████████████] 1452/1452 Loss: 2.3830\n",
      "Epoch 3/100 finished. Avg Train Loss: 2.4213 | Val Loss: 2.4713\n",
      "Epoch 4/100 | [██████████████████████████████] 1452/1452 Loss: 2.4200\n",
      "Epoch 4/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4712\n",
      "Epoch 5/100 | [██████████████████████████████] 1452/1452 Loss: 2.4333\n",
      "Epoch 5/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4751\n",
      "Epoch 6/100 | [██████████████████████████████] 1452/1452 Loss: 2.4040\n",
      "Epoch 6/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4735\n",
      "Epoch 7/100 | [██████████████████████████████] 1452/1452 Loss: 2.4227\n",
      "Epoch 7/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4745\n",
      "Epoch 8/100 | [██████████████████████████████] 1452/1452 Loss: 2.4412\n",
      "Epoch 8/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4706\n",
      "Epoch 9/100 | [██████████████████████████████] 1452/1452 Loss: 2.4244\n",
      "Epoch 9/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4751\n",
      "Epoch 10/100 | [██████████████████████████████] 1452/1452 Loss: 2.3925\n",
      "Epoch 10/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4737\n",
      "Epoch 11/100 | [██████████████████████████████] 1452/1452 Loss: 2.4102\n",
      "Epoch 11/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4733\n",
      "Epoch 12/100 | [██████████████████████████████] 1452/1452 Loss: 2.4167\n",
      "Epoch 12/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4725\n",
      "Epoch 13/100 | [██████████████████████████████] 1452/1452 Loss: 2.4345\n",
      "Epoch 13/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4748\n",
      "Epoch 14/100 | [██████████████████████████████] 1452/1452 Loss: 2.3911\n",
      "Epoch 14/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 15/100 | [██████████████████████████████] 1452/1452 Loss: 2.3983\n",
      "Epoch 15/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4716\n",
      "Epoch 16/100 | [██████████████████████████████] 1452/1452 Loss: 2.4361\n",
      "Epoch 16/100 finished. Avg Train Loss: 2.4211 | Val Loss: 2.4731\n",
      "Epoch 17/100 | [██████████████████████████████] 1452/1452 Loss: 2.3809\n",
      "Epoch 17/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4752\n",
      "Epoch 18/100 | [██████████████████████████████] 1452/1452 Loss: 2.4185\n",
      "Epoch 18/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4724\n",
      "Epoch 19/100 | [██████████████████████████████] 1452/1452 Loss: 2.3945\n",
      "Epoch 19/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4747\n",
      "Epoch 20/100 | [██████████████████████████████] 1452/1452 Loss: 2.4143\n",
      "Epoch 20/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4720\n",
      "Epoch 21/100 | [██████████████████████████████] 1452/1452 Loss: 2.4134\n",
      "Epoch 21/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4741\n",
      "Epoch 22/100 | [██████████████████████████████] 1452/1452 Loss: 2.4520\n",
      "Epoch 22/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4730\n",
      "Epoch 23/100 | [██████████████████████████████] 1452/1452 Loss: 2.3764\n",
      "Epoch 23/100 finished. Avg Train Loss: 2.4190 | Val Loss: 2.4750\n",
      "Epoch 24/100 | [██████████████████████████████] 1452/1452 Loss: 2.4000\n",
      "Epoch 24/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4732\n",
      "Epoch 25/100 | [██████████████████████████████] 1452/1452 Loss: 2.4173\n",
      "Epoch 25/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4721\n",
      "Epoch 26/100 | [██████████████████████████████] 1452/1452 Loss: 2.4294\n",
      "Epoch 26/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4718\n",
      "Epoch 27/100 | [██████████████████████████████] 1452/1452 Loss: 2.4401\n",
      "Epoch 27/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4704\n",
      "Epoch 28/100 | [██████████████████████████████] 1452/1452 Loss: 2.4478\n",
      "Epoch 28/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4732\n",
      "Epoch 29/100 | [██████████████████████████████] 1452/1452 Loss: 2.4223\n",
      "Epoch 29/100 finished. Avg Train Loss: 2.4192 | Val Loss: 2.4723\n",
      "Epoch 30/100 | [██████████████████████████████] 1452/1452 Loss: 2.4337\n",
      "Epoch 30/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4733\n",
      "Epoch 31/100 | [██████████████████████████████] 1452/1452 Loss: 2.4342\n",
      "Epoch 31/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4729\n",
      "Epoch 32/100 | [██████████████████████████████] 1452/1452 Loss: 2.4275\n",
      "Epoch 32/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4716\n",
      "Epoch 33/100 | [██████████████████████████████] 1452/1452 Loss: 2.4209\n",
      "Epoch 33/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4722\n",
      "Epoch 34/100 | [██████████████████████████████] 1452/1452 Loss: 2.3950\n",
      "Epoch 34/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4734\n",
      "Epoch 35/100 | [██████████████████████████████] 1452/1452 Loss: 2.4104\n",
      "Epoch 35/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4731\n",
      "Epoch 36/100 | [██████████████████████████████] 1452/1452 Loss: 2.4188\n",
      "Epoch 36/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4726\n",
      "Epoch 37/100 | [██████████████████████████████] 1452/1452 Loss: 2.3964\n",
      "Epoch 37/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4718\n",
      "Epoch 38/100 | [██████████████████████████████] 1452/1452 Loss: 2.4169\n",
      "Epoch 38/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4708\n",
      "Epoch 39/100 | [██████████████████████████████] 1452/1452 Loss: 2.4353\n",
      "Epoch 39/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4722\n",
      "Epoch 40/100 | [██████████████████████████████] 1452/1452 Loss: 2.4280\n",
      "Epoch 40/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4713\n",
      "Epoch 41/100 | [██████████████████████████████] 1452/1452 Loss: 2.4357\n",
      "Epoch 41/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4719\n",
      "Epoch 42/100 | [██████████████████████████████] 1452/1452 Loss: 2.4290\n",
      "Epoch 42/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4721\n",
      "Epoch 43/100 | [██████████████████████████████] 1452/1452 Loss: 2.4311\n",
      "Epoch 43/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4736\n",
      "Epoch 44/100 | [██████████████████████████████] 1452/1452 Loss: 2.4357\n",
      "Epoch 44/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4743\n",
      "Epoch 45/100 | [██████████████████████████████] 1452/1452 Loss: 2.4064\n",
      "Epoch 45/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4718\n",
      "Epoch 46/100 | [██████████████████████████████] 1452/1452 Loss: 2.4345\n",
      "Epoch 46/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4742\n",
      "Epoch 47/100 | [██████████████████████████████] 1452/1452 Loss: 2.4235\n",
      "Epoch 47/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4712\n",
      "Epoch 48/100 | [██████████████████████████████] 1452/1452 Loss: 2.4269\n",
      "Epoch 48/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4740\n",
      "Epoch 49/100 | [██████████████████████████████] 1452/1452 Loss: 2.4272\n",
      "Epoch 49/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4754\n",
      "Epoch 50/100 | [██████████████████████████████] 1452/1452 Loss: 2.4543\n",
      "Epoch 50/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4712\n",
      "Epoch 51/100 | [██████████████████████████████] 1452/1452 Loss: 2.4565\n",
      "Epoch 51/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4729\n",
      "Epoch 52/100 | [██████████████████████████████] 1452/1452 Loss: 2.4091\n",
      "Epoch 52/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4728\n",
      "Epoch 53/100 | [██████████████████████████████] 1452/1452 Loss: 2.4547\n",
      "Epoch 53/100 finished. Avg Train Loss: 2.4194 | Val Loss: 2.4709\n",
      "Epoch 54/100 | [██████████████████████████████] 1452/1452 Loss: 2.3999\n",
      "Epoch 54/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 55/100 | [██████████████████████████████] 1452/1452 Loss: 2.4302\n",
      "Epoch 55/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4730\n",
      "Epoch 56/100 | [██████████████████████████████] 1452/1452 Loss: 2.4618\n",
      "Epoch 56/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4725\n",
      "Epoch 57/100 | [██████████████████████████████] 1452/1452 Loss: 2.4265\n",
      "Epoch 57/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4718\n",
      "Epoch 58/100 | [██████████████████████████████] 1452/1452 Loss: 2.4388\n",
      "Epoch 58/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4747\n",
      "Epoch 59/100 | [██████████████████████████████] 1452/1452 Loss: 2.4115\n",
      "Epoch 59/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4713\n",
      "Epoch 60/100 | [██████████████████████████████] 1452/1452 Loss: 2.3903\n",
      "Epoch 60/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4745\n",
      "Epoch 61/100 | [██████████████████████████████] 1452/1452 Loss: 2.4113\n",
      "Epoch 61/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4731\n",
      "Epoch 62/100 | [██████████████████████████████] 1452/1452 Loss: 2.4378\n",
      "Epoch 62/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4726\n",
      "Epoch 63/100 | [██████████████████████████████] 1452/1452 Loss: 2.3946\n",
      "Epoch 63/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4750\n",
      "Epoch 64/100 | [██████████████████████████████] 1452/1452 Loss: 2.4441\n",
      "Epoch 64/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4755\n",
      "Epoch 65/100 | [██████████████████████████████] 1452/1452 Loss: 2.3982\n",
      "Epoch 65/100 finished. Avg Train Loss: 2.4190 | Val Loss: 2.4705\n",
      "Epoch 66/100 | [██████████████████████████████] 1452/1452 Loss: 2.4261\n",
      "Epoch 66/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4723\n",
      "Epoch 67/100 | [██████████████████████████████] 1452/1452 Loss: 2.4688\n",
      "Epoch 67/100 finished. Avg Train Loss: 2.4206 | Val Loss: 2.4746\n",
      "Epoch 68/100 | [██████████████████████████████] 1452/1452 Loss: 2.4188\n",
      "Epoch 68/100 finished. Avg Train Loss: 2.4211 | Val Loss: 2.4725\n",
      "Epoch 69/100 | [██████████████████████████████] 1452/1452 Loss: 2.4051\n",
      "Epoch 69/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4730\n",
      "Epoch 70/100 | [██████████████████████████████] 1452/1452 Loss: 2.4202\n",
      "Epoch 70/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4727\n",
      "Epoch 71/100 | [██████████████████████████████] 1452/1452 Loss: 2.4331\n",
      "Epoch 71/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4728\n",
      "Epoch 72/100 | [██████████████████████████████] 1452/1452 Loss: 2.4299\n",
      "Epoch 72/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4712\n",
      "Epoch 73/100 | [██████████████████████████████] 1452/1452 Loss: 2.4302\n",
      "Epoch 73/100 finished. Avg Train Loss: 2.4192 | Val Loss: 2.4739\n",
      "Epoch 74/100 | [██████████████████████████████] 1452/1452 Loss: 2.4004\n",
      "Epoch 74/100 finished. Avg Train Loss: 2.4195 | Val Loss: 2.4701\n",
      "Epoch 75/100 | [██████████████████████████████] 1452/1452 Loss: 2.4398\n",
      "Epoch 75/100 finished. Avg Train Loss: 2.4193 | Val Loss: 2.4731\n",
      "Epoch 76/100 | [██████████████████████████████] 1452/1452 Loss: 2.4658\n",
      "Epoch 76/100 finished. Avg Train Loss: 2.4210 | Val Loss: 2.4739\n",
      "Epoch 77/100 | [██████████████████████████████] 1452/1452 Loss: 2.3873\n",
      "Epoch 77/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4738\n",
      "Epoch 78/100 | [██████████████████████████████] 1452/1452 Loss: 2.3988\n",
      "Epoch 78/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4707\n",
      "Epoch 79/100 | [██████████████████████████████] 1452/1452 Loss: 2.4732\n",
      "Epoch 79/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4722\n",
      "Epoch 80/100 | [██████████████████████████████] 1452/1452 Loss: 2.4205\n",
      "Epoch 80/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4744\n",
      "Epoch 81/100 | [██████████████████████████████] 1452/1452 Loss: 2.3921\n",
      "Epoch 81/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4730\n",
      "Epoch 82/100 | [██████████████████████████████] 1452/1452 Loss: 2.3853\n",
      "Epoch 82/100 finished. Avg Train Loss: 2.4207 | Val Loss: 2.4735\n",
      "Epoch 83/100 | [██████████████████████████████] 1452/1452 Loss: 2.4233\n",
      "Epoch 83/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4713\n",
      "Epoch 84/100 | [██████████████████████████████] 1452/1452 Loss: 2.4057\n",
      "Epoch 84/100 finished. Avg Train Loss: 2.4196 | Val Loss: 2.4748\n",
      "Epoch 85/100 | [██████████████████████████████] 1452/1452 Loss: 2.4266\n",
      "Epoch 85/100 finished. Avg Train Loss: 2.4198 | Val Loss: 2.4754\n",
      "Epoch 86/100 | [██████████████████████████████] 1452/1452 Loss: 2.4035\n",
      "Epoch 86/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4722\n",
      "Epoch 87/100 | [██████████████████████████████] 1452/1452 Loss: 2.3928\n",
      "Epoch 87/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4727\n",
      "Epoch 88/100 | [██████████████████████████████] 1452/1452 Loss: 2.4315\n",
      "Epoch 88/100 finished. Avg Train Loss: 2.4208 | Val Loss: 2.4727\n",
      "Epoch 89/100 | [██████████████████████████████] 1452/1452 Loss: 2.4067\n",
      "Epoch 89/100 finished. Avg Train Loss: 2.4197 | Val Loss: 2.4725\n",
      "Epoch 90/100 | [██████████████████████████████] 1452/1452 Loss: 2.4515\n",
      "Epoch 90/100 finished. Avg Train Loss: 2.4200 | Val Loss: 2.4719\n",
      "Epoch 91/100 | [██████████████████████████████] 1452/1452 Loss: 2.4277\n",
      "Epoch 91/100 finished. Avg Train Loss: 2.4204 | Val Loss: 2.4710\n",
      "Epoch 92/100 | [██████████████████████████████] 1452/1452 Loss: 2.4230\n",
      "Epoch 92/100 finished. Avg Train Loss: 2.4205 | Val Loss: 2.4715\n",
      "Epoch 93/100 | [██████████████████████████████] 1452/1452 Loss: 2.3932\n",
      "Epoch 93/100 finished. Avg Train Loss: 2.4202 | Val Loss: 2.4717\n",
      "Epoch 94/100 | [██████████████████████████████] 1452/1452 Loss: 2.4171\n",
      "Epoch 94/100 finished. Avg Train Loss: 2.4201 | Val Loss: 2.4727\n",
      "Epoch 95/100 | [██████████████████████████████] 1452/1452 Loss: 2.4279\n",
      "Epoch 95/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4738\n",
      "Epoch 96/100 | [██████████████████████████████] 1452/1452 Loss: 2.4075\n",
      "Epoch 96/100 finished. Avg Train Loss: 2.4199 | Val Loss: 2.4748\n",
      "Epoch 97/100 | [██████████████████████████████] 1452/1452 Loss: 2.4509\n",
      "Epoch 97/100 finished. Avg Train Loss: 2.4209 | Val Loss: 2.4729\n",
      "Epoch 98/100 | [██████████████████████████████] 1452/1452 Loss: 2.4218\n",
      "Epoch 98/100 finished. Avg Train Loss: 2.4203 | Val Loss: 2.4720\n",
      "Epoch 99/100 | [██████████████████████████████] 1452/1452 Loss: 2.4091\n",
      "Epoch 99/100 finished. Avg Train Loss: 2.4218 | Val Loss: 2.4738\n",
      "Epoch 100/100 | [██████████████████████████████] 1452/1452 Loss: 2.4140\n",
      "Epoch 100/100 finished. Avg Train Loss: 2.4193 | Val Loss: 2.4730\n"
     ]
    }
   ],
   "source": [
    "train_BLM(100, model, train_data, val_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eed9c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASheekeatos.\n",
      "\n",
      "ar I'verepralorear wemucorecr.\n",
      "\n",
      "y. ay w fanow bed w heth hawo ome be fored \"braft. heat s-chabe carches, whim tinersooawswhof skety lea p wabliz it be be esine chess THed boflinawesmeasite yin athe\n",
      "ally bucarem thed t rves awid fo warey\n",
      "\n",
      "\"\n",
      "lot m.\n",
      "uro myo beas.\"ale gsklay,\" ie thewin as ca she plin t to the bbluth, itint\n",
      "grs t\n",
      "\n",
      "s ugoufas rrslller t o aulerizad oupll waboulld arn iofofed fle, id s\n",
      "thagairs aweermappllem, sur the s fotca o narlaimizagr hs se coocablan sived wifothed \n"
     ]
    }
   ],
   "source": [
    "# Prepare context\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "# Use .module.generate if DataParallel is active\n",
    "if isinstance(m, torch.nn.DataParallel):\n",
    "    generatedChars = decode(m.module.generate(context, max_new_tokens=500)[0].tolist())\n",
    "else:\n",
    "    generatedChars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generatedChars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
